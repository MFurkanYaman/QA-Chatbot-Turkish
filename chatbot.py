# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_HC2-uBncQQ2WNhEyaystHWo60rD5dN4
"""

!pip install langchain huggingface_hub sentence-transformers
!pip install langchain_community
!pip install sentence-transformers faiss-cpu transformers keybert tqdm
!pip install tqdm
!pip install pprint
!pip install faiss
!pip install keybert
!pip install transformers
!pip install llama-index
!pip install langchain
!pip install PyMuPDF
!pip install gpt2
!pip install llama_index
!pip install --upgrade sentence-transformers
# %pip install llama-index-vector-stores-faiss

import os
import faiss
import json
import fitz
import gpt2
import pandas as pd

from llama_index.core import SimpleDirectoryReader
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, GPT2Model, GPT2Tokenizer
from tqdm import tqdm
from keybert import KeyBERT
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import TokenTextSplitter
from pprint import pprint
from llama_index.legacy.embeddings.langchain import LangchainEmbedding
from langchain.text_splitter import TextSplitter
from google.colab import drive

drive.mount('/content/drive')


df = pd.read_csv('/content/drive/MyDrive/veri/yeni_veriseti.csv')

questions = df['questions'].tolist()
answers = df['answers'].tolist()

embed_model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
embed_model = SentenceTransformer(embed_model_name)
tokenizer = AutoTokenizer.from_pretrained(embed_model_name)
kw_model = KeyBERT()

question_embeddings = embed_model.encode(questions, show_progress_bar=True)

embedding_dimension = question_embeddings.shape[1]
faiss_index = faiss.IndexHNSWFlat(embedding_dimension, 32)
faiss_index.hnsw.efConstruction = 40
faiss_index.hnsw.efSearch = 16
faiss_index.add(question_embeddings)

class TextSplitter:
    def __init__(self, chunk_size=512, chunk_overlap=20):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text):
        tokens = tokenizer.encode(text)
        chunks = []
        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):
            chunk = tokens[i:i + self.chunk_size]
            chunks.append(tokenizer.decode(chunk))
        return chunks

text_splitter = TextSplitter(chunk_size=512, chunk_overlap=20)

def get_answer(query: str, similarity_top_k: int = 1):
    query_embedding = embed_model.encode([query])
    distances, indices = faiss_index.search(query_embedding, k=similarity_top_k)
    result_answers = [answers[i] for i in indices[0]]
    result = {"answer": result_answers, "distances": distances[0]}
    return result

query = "Bilgi Teknolojileri biriminin yaptığı projeler hakkında bilgi ver."
result = get_answer(query)

for answer in result['answer']:
    print("Kullanıcı: ",query)
    print()
    print("Chatbot: ",answer)

